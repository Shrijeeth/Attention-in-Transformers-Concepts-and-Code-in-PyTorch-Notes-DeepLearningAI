{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d5a11a8",
   "metadata": {},
   "source": [
    "# Coding Encoder–Decoder Attention and Multi‑Head Attention in PyTorch\n",
    "\n",
    "A hands-on lesson that implements generic attention (self/cross) and a simple multi‑head attention wrapper, with explanations, shapes, and references.\n",
    "\n",
    "![Transformer architecture diagram (Wikimedia Commons)](https://commons.wikimedia.org/wiki/Special:FilePath/Transformer%2C_full_architecture.png)\n",
    "\n",
    "- Paper: [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
    "- Visual guide: [The Illustrated Transformer (Jay Alammar)](https://jalammar.github.io/illustrated-transformer/)\n",
    "- Course: [Hugging Face — Transformer Architectures](https://huggingface.co/learn/llm-course/en/chapter1/6)\n",
    "- Docs: [PyTorch nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d99a62",
   "metadata": {},
   "source": [
    "## Imports and prerequisites\n",
    "\n",
    "We use PyTorch to build generic attention (self or cross) and a simple multi‑head wrapper:\n",
    "\n",
    "- `torch`: tensors and linear algebra helpers.\n",
    "- `torch.nn` (`nn`): layers like `Linear` and base class `Module`.\n",
    "- `torch.nn.functional` (`F`): stateless ops like `softmax` used in attention.\n",
    "\n",
    "Note: Tensors are multi‑dimensional arrays optimized for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e27608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a45c2f3",
   "metadata": {},
   "source": [
    "## Generic Attention (self or cross)\n",
    "\n",
    "This module can perform self-attention (Q=K=V from the same encodings) or cross-attention (Q from decoder, K/V from encoder).\n",
    "\n",
    "### Init arguments\n",
    "\n",
    "| Argument | Meaning | Why it matters |\n",
    "| --- | --- | --- |\n",
    "| `d_model` | Features per token (embedding width) | Sets sizes of `W_q`, `W_k`, `W_v` |\n",
    "| `row_dim` | Axis for rows in transposes | Controls `K.T` behavior |\n",
    "| `col_dim` | Axis for columns/softmax | Controls softmax dimension and sqrt(d_k) extraction |\n",
    "\n",
    "### Forward (single head)\n",
    "\n",
    "```text\n",
    "q = W_q(encodings_for_q)\n",
    "k = W_k(encodings_for_k)\n",
    "v = W_v(encodings_for_v)\n",
    "S = q · k^T\n",
    "S_scaled = S / sqrt(d_k)\n",
    "if mask:\n",
    "  S_scaled = S_scaled.masked_fill(mask, -1e9)\n",
    "A = softmax(S_scaled, dim=col_dim)\n",
    "O = A · v\n",
    "```\n",
    "\n",
    "### Shapes\n",
    "\n",
    "| Tensor | Shape |\n",
    "| --- | --- |\n",
    "| `encodings_for_q` | `n_q × d_model` |\n",
    "| `encodings_for_k`, `encodings_for_v` | `n_kv × d_model` |\n",
    "| `q`, `k`, `v` | `n_* × d_model` (here) |\n",
    "| `S = qk^T` | `n_q × n_kv` |\n",
    "| `A` | `n_q × n_kv` |\n",
    "| `O = Av` | `n_q × d_model` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa03daab",
   "metadata": {},
   "source": [
    "## Sample inputs: self vs cross attention\n",
    "\n",
    "For self-attention, we set `encodings_for_q = encodings_for_k = encodings_for_v`.\n",
    "This keeps the demo simple and lets you verify shapes by hand.\n",
    "\n",
    "- Shape here: each is `n × d_model = 3 × 2`.\n",
    "- In cross-attention, you would pass different sources for `encodings_for_q` (decoder states) versus `encodings_for_k, encodings_for_v` (encoder outputs).\n",
    "\n",
    "| Tensor | Meaning | Shape |\n",
    "| --- | --- | --- |\n",
    "| `encodings_for_q` | Inputs for queries | `n_q × d_model` |\n",
    "| `encodings_for_k` | Inputs for keys | `n_kv × d_model` |\n",
    "| `encodings_for_v` | Inputs for values | `n_kv × d_model` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478a1de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=2, row_dim=0, col_dim=1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c90d1e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_for_q = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_k = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_v = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c1974a",
   "metadata": {},
   "source": [
    "## Multi‑Head Attention (simple wrapper)\n",
    "\n",
    "We wrap multiple single‑head attention modules and concatenate their outputs along the feature dimension.\n",
    "\n",
    "### Idea\n",
    "\n",
    "- Each head has its own `W_q, W_k, W_v` and computes scaled dot‑product attention independently.\n",
    "- We concatenate per‑head outputs: `O_cat = concat(O_1, …, O_H)`.\n",
    "- In full Transformers, a learned projection `W_O` maps `O_cat → d_model`. Here we focus on concatenation to see the effect of multiple heads.\n",
    "\n",
    "### Shapes (typical)\n",
    "\n",
    "| Item | Shape |\n",
    "| --- | --- |\n",
    "| Input `X` | `n × d_model` |\n",
    "| Per‑head `Q_h, K_h, V_h` | `n × d_k`, `n × d_k`, `n × d_v` |\n",
    "| Per‑head output `O_h` | `n × d_v` |\n",
    "| Concatenated `O_cat` | `n × (H·d_v)` |\n",
    "| Output projection `W_O` (not shown here) | `(H·d_v) × d_model` |\n",
    "\n",
    "This mirrors the original paper’s MHA block while keeping code minimal for teaching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbdfa3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "attention = Attention(d_model=2, row_dim=0, col_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe5817b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_output = attention(encodings_for_q, encodings_for_k, encodings_for_v)\n",
    "original_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca3f12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=2, row_dim=0, col_dim=1, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                Attention(d_model, row_dim, col_dim)\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v):\n",
    "        return torch.cat([\n",
    "            head(encodings_for_q, encodings_for_k, encodings_for_v)\n",
    "            for head in self.heads\n",
    "        ], dim=self.col_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0691a23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "multi_head_attention = MultiHeadAttention(\n",
    "    d_model=2,\n",
    "    row_dim=0,\n",
    "    col_dim=1,\n",
    "    num_heads=1\n",
    ")\n",
    "\n",
    "original_output_2 = multi_head_attention(encodings_for_q, encodings_for_k, encodings_for_v)\n",
    "original_output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a77b3ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0100,  1.0641, -0.7081, -0.8268],\n",
       "        [ 0.2040,  0.7057, -0.7417, -0.9193],\n",
       "        [ 3.4989,  2.2427, -0.7190, -0.8447]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "multi_head_attention = MultiHeadAttention(\n",
    "    d_model=2,\n",
    "    row_dim=0,\n",
    "    col_dim=1,\n",
    "    num_heads=2\n",
    ")\n",
    "\n",
    "original_output_3 = multi_head_attention(encodings_for_q, encodings_for_k, encodings_for_v)\n",
    "original_output_3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
