{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8245581",
   "metadata": {},
   "source": [
    "# Coding Self-Attention in PyTorch\n",
    "\n",
    "A hands-on lesson to implement scaled dot-product self-attention in PyTorch, verify calculations numerically, and connect the math to the code.\n",
    "\n",
    "![Transformer architecture diagram (Wikimedia Commons)](https://commons.wikimedia.org/wiki/Special:FilePath/Transformer%2C_full_architecture.png)\n",
    "\n",
    "- Paper: [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
    "- Visual guide: [The Illustrated Transformer (Jay Alammar)](https://jalammar.github.io/illustrated-transformer/)\n",
    "- PyTorch docs: [nn.MultiheadAttention](https://docs.pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896dac8f",
   "metadata": {},
   "source": [
    "## Imports and prerequisites\n",
    "\n",
    "In this lesson, we use PyTorch to build and run self-attention:\n",
    "\n",
    "- `torch`: tensors and linear algebra helpers.\n",
    "- `torch.nn` (`nn`): neural network layers like `Linear` and base class `Module`.\n",
    "- `torch.nn.functional` (`F`): stateless ops like `softmax` used in attention.\n",
    "\n",
    "Note: Tensors are multi-dimensional arrays optimized for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ceba401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0b53a",
   "metadata": {},
   "source": [
    "## Implementing the SelfAttention class\n",
    "\n",
    "We implement a self-attention module as a standard `nn.Module` so it plugs into PyTorch pipelines.\n",
    "\n",
    "### __init__ parameters\n",
    "\n",
    "| Parameter | Meaning | Why it matters |\n",
    "| --- | --- | --- |\n",
    "| `d_model` | Number of features per token (embedding size after positions) | Sets the sizes of weight matrices for Q/K/V |\n",
    "| `row_dim` | Axis used for rows in matrix ops | Controls how we transpose during `QK^T` |\n",
    "| `col_dim` | Axis used for columns in matrix ops | Controls softmax axis and matmul behavior |\n",
    "\n",
    "- We create three `nn.Linear` layers without bias (`bias=False`) to realize the learnable matrices `W_q`, `W_k`, and `W_v`.\n",
    "- Following the original Transformer, attention projections typically omit bias terms.\n",
    "- We store `row_dim` and `col_dim` for flexible batching later (here we use a simple 2D example without batches).\n",
    "\n",
    "### Forward pass sketch\n",
    "\n",
    "```text\n",
    "q = W_q(X)\n",
    "k = W_k(X)\n",
    "v = W_v(X)\n",
    "S = q · k^T\n",
    "S_scaled = S / sqrt(d_k)\n",
    "A = softmax(S_scaled, dim=col_dim)\n",
    "O = A · v\n",
    "```\n",
    "\n",
    "This matches the scaled dot-product attention described in Vaswani et al. (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d12eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model=2, row_dim=0, col_dim=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(\n",
    "            in_features=d_model,\n",
    "            out_features=d_model,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.W_k = nn.Linear(\n",
    "            in_features=d_model,\n",
    "            out_features=d_model,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.W_v = nn.Linear(\n",
    "            in_features=d_model,\n",
    "            out_features=d_model,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "\n",
    "    def forward(self, token_encodings):\n",
    "        q = self.W_q(token_encodings)\n",
    "        k = self.W_k(token_encodings)\n",
    "        v = self.W_v(token_encodings)\n",
    "\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48353d7b",
   "metadata": {},
   "source": [
    "## Sample token encodings (toy example)\n",
    "\n",
    "We’ll work with a tiny 2D encoding per token so we can verify all math by hand.\n",
    "\n",
    "- Shape: `encodings_matrix` is `n_tokens × d_model = 3 × 2`.\n",
    "- In practice, `d_model` is often 256–4096+, but 2D keeps examples simple.\n",
    "\n",
    "| Token index | Encoded values (example) |\n",
    "| --- | --- |\n",
    "| 0 | [1.16, 0.23] |\n",
    "| 1 | [0.57, 1.36] |\n",
    "| 2 | [4.41, -2.16] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74beb24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_matrix = torch.tensor([\n",
    "    [1.16, 0.23],\n",
    "    [0.57, 1.36],\n",
    "    [4.41, -2.16],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be012a",
   "metadata": {},
   "source": [
    "## Seeding and instantiation\n",
    "\n",
    "We create a reproducible run and instantiate our self-attention module.\n",
    "\n",
    "- `torch.manual_seed(42)`: sets the random seed so weights and results are repeatable.\n",
    "- `SelfAttention(d_model=2, row_dim=0, col_dim=1)`: tiny model (2 features/token) for hand-checking math.\n",
    "\n",
    "| Argument | Value | Purpose |\n",
    "| --- | --- | --- |\n",
    "| `d_model` | 2 | Features per token; sets sizes of `W_q`, `W_k`, `W_v` |\n",
    "| `row_dim` | 0 | Row axis used in `k.transpose` inside `QK^T` |\n",
    "| `col_dim` | 1 | Column/feature axis; softmax is applied along this axis |\n",
    "\n",
    "Tip: In batched settings you’d add a batch dimension and keep `row_dim`/`col_dim` consistent with tensor layout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb63845",
   "metadata": {},
   "source": [
    "## Shapes and intermediate tensors\n",
    "\n",
    "With `n = 3` tokens and `d_model = d_k = d_v = 2`, the shapes are:\n",
    "\n",
    "| Tensor | Meaning | Shape |\n",
    "| --- | --- | --- |\n",
    "| `X` | Encoded tokens (embeddings + positions) | 3 × 2 |\n",
    "| `q = W_q(X)` | Queries | 3 × 2 |\n",
    "| `k = W_k(X)` | Keys | 3 × 2 |\n",
    "| `v = W_v(X)` | Values | 3 × 2 |\n",
    "| `sims = q · k^T` | Unscaled similarities | 3 × 3 |\n",
    "| `scaled_sims = sims / sqrt(d_k)` | Scaled similarities | 3 × 3 |\n",
    "| `attention_percents = softmax(scaled_sims, dim=1)` | Attention weights (rows sum to 1) | 3 × 3 |\n",
    "| `attention_scores = attention_percents · v` | Output (context-aware) | 3 × 2 |\n",
    "\n",
    "The next cell runs the full forward pass and returns `attention_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e84637db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "self_attention = SelfAttention(\n",
    "    d_model=2,\n",
    "    row_dim=0,\n",
    "    col_dim=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06f950d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_output = self_attention(encodings_matrix)\n",
    "original_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76de1fe",
   "metadata": {},
   "source": [
    "### Validate Weights and Manual Calculations\n",
    "\n",
    "Now we’ll inspect the learned (randomly initialized) weights and validate the math.\n",
    "\n",
    "- We transpose `weight` for readability because of how PyTorch stores/prints linear layer weights.\n",
    "- By combining the printed `W_q`, `W_k`, `W_v` with the original encodings, you can recompute Q, K, V and verify each step by hand.\n",
    "- This confirms our implementation matches the scaled dot-product attention math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bff806d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5406, -0.1657],\n",
       "        [ 0.5869,  0.6496]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention.W_q.weight.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b0f7b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1549, -0.3443],\n",
       "        [ 0.1427,  0.4153]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention.W_k.weight.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a248e42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6233,  0.6146],\n",
       "        [-0.5188,  0.1323]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention.W_v.weight.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a373ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7621, -0.0428],\n",
       "        [ 1.1063,  0.7890],\n",
       "        [ 1.1164, -2.1336]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = self_attention.W_q(encodings_matrix)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00dfd2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1469, -0.3038],\n",
       "        [ 0.1057,  0.3685],\n",
       "        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = self_attention.W_k(encodings_matrix)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96dbe4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6038,  0.7434],\n",
       "        [-0.3502,  0.5303],\n",
       "        [ 3.8695,  2.4246]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = self_attention.W_v(encodings_matrix)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a41be3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0990,  0.0648, -0.6523],\n",
       "        [-0.4022,  0.4078, -3.0024],\n",
       "        [ 0.4842, -0.6683,  4.0461]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = torch.matmul(q, k.transpose(dim0=0, dim1=1))\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f5fa3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0700,  0.0458, -0.4612],\n",
       "        [-0.2844,  0.2883, -2.1230],\n",
       "        [ 0.3424, -0.4725,  2.8610]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_sims = sims / (torch.tensor(2)**0.5)\n",
    "scaled_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfb5fe94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3573, 0.4011, 0.2416],\n",
       "        [0.3410, 0.6047, 0.0542],\n",
       "        [0.0722, 0.0320, 0.8959]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_percents = F.softmax(scaled_sims, dim=1)\n",
    "attention_percents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47c58a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output = torch.matmul(attention_percents, self_attention.W_v(encodings_matrix))\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3dd0685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output == original_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a440b8f",
   "metadata": {},
   "source": [
    "## References and further reading\n",
    "\n",
    "- Vaswani et al., 2017 — [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- Jay Alammar — [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "- PyTorch — [nn.MultiheadAttention docs](https://docs.pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)\n",
    "- UvA DL — [Transformers and Multi-Head Attention](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)\n",
    "- Interactive — [Transformer Explainer (GPT‑2 attention)](https://poloclub.github.io/transformer-explainer/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
